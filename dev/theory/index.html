<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory · ImplicitAD.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://byuflowlab.github.io/ImplicitAD.jl/theory/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ImplicitAD.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../reference/">API</a></li><li class="is-active"><a class="tocitem" href>Theory</a><ul class="internal"><li><a class="tocitem" href="#Implicit-Equations"><span>Implicit Equations</span></a></li><li><a class="tocitem" href="#Linear-Equations"><span>Linear Equations</span></a></li><li><a class="tocitem" href="#Custom-Rules"><span>Custom Rules</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theory</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/byuflowlab/ImplicitAD.jl/blob/main/docs/src/theory.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h1><h2 id="Implicit-Equations"><a class="docs-heading-anchor" href="#Implicit-Equations">Implicit Equations</a><a id="Implicit-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-Equations" title="Permalink"></a></h2><p>We can get to the derivatives we are after by using implicit differentiation. Recall that we are solving: <span>$r(x, y(x)) = 0$</span> and that we want <span>$dy/dx$</span>.  Using the chain rule we find that:</p><p class="math-container">\[\frac{dr}{dx} = \frac{\partial r}{\partial x} + \frac{\partial r}{\partial y} \frac{dy}{dx} = 0\]</p><p>To be a properly solution, the residuals must remain at <span>$r = 0$</span> with changes in the input.  Thus, the total derivative <span>$dr/dx$</span> must also be zero.</p><p>The Jacobian we are after we will call <span>$J = \frac{dy}{dx}$</span>.  The other two partial derivative Jacobians we will call <span>$A = \frac{\partial r}{\partial y}$</span> and <span>$B = \frac{\partial r}{\partial x}$</span> just for convenience.  The above equation then gives:</p><p class="math-container">\[A J = -B\]</p><p>Note that our desired Jacobian depends only on partial derivatives at the solution of our residual equations (and thus not on the solver path).  We can compute or provide these partial derivatives by various means, though typically we will leverage AD.</p><h4 id="Forward-Mode"><a class="docs-heading-anchor" href="#Forward-Mode">Forward Mode</a><a id="Forward-Mode-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Mode" title="Permalink"></a></h4><p>If we are using forward mode algorithmic differentiation, once we reach this implicit function we already know the derivatives of x with respect to our variables of interest.  We will call this <span>$\dot{x}$</span>.  The above equation described how we can compute the Jacobian <span>$J$</span>, but what we really want is not <span>$J$</span> but rather the Jacobian vector product <span>$J \dot{x}$</span>, which gives us the derivatives of y with respect to our variables of interest, and we continue forward along the AD path.</p><p>We multiply both sides by <span>$\dot{x}$</span></p><p class="math-container">\[A J \dot{x} = -B \dot{x}\]</p><p>or </p><p class="math-container">\[A \dot{y} = -B \dot{x}\]</p><p>where <span>$\dot{y}$</span> is what we are after.</p><p>First we deal with the right hand side computing:</p><p class="math-container">\[b = -B \dot{x}\]</p><p>We can compute the resulting Jacobian vector product (Jacobian B times some vector v) without actually forming the matrix B, which is more efficient.  Recall that <span>$B = \partial r/\partial x$</span>.  If we want the Jacobian product <span>$B v$</span>, where <span>$v = -\dot{x}$</span> in this case, we see that we are after a weighted sum of the columns <span>$\sum_i \partial r/\partial x_i v_i$</span> where <span>$v_i$</span> are the weights.  Normally we pick off one of these partials in forward mode AD one at a time with different seeds <span>$e_i = [0, 0, 1, 0]$</span>, but if we set the initial seed to <span>$v_i$</span> then the partial derivatives we compute are precisely this weighted sum.  In other words, we initialize <span>$x$</span> as a dual number with its values as the input value <span>$x$</span>, and its derivatives as <span>$v$</span>, we evalute the residual function, then extract the partial derivatives which now give the desired Jacobian vector product <span>$b = B v$</span> (where <span>$v = -\dot{x}$</span>).</p><p>With the right-hand side complete, we just need to compute the square Jacobian <span>$A = \partial r / \partial y$</span> and solve the linear system.  </p><p class="math-container">\[A \dot{y} = b\]</p><p>Because A is square, forward mode AD is usually preferable for computing these partial derivatives. If we know the structure of A, we would want to provide an appropriate factorization of A.  Often, this Jacobian is sparse, and so using graph coloring is desirable.  Also for very large systems we can use matrix-free methods (e.g., Krylov methods) to solve the linear system without actually constructing A.</p><p>In summary the operations are:</p><ol><li>Compute the JVP: <span>$b = -B \dot{x}$</span> for the upstream input <span>$\dot{x}$</span> where <span>$B = \partial r/\partial x$</span>.</li><li>Solve the linear system: <span>$A \dot{y} = b$</span> where <span>$A = \partial r / \partial y$</span>.</li></ol><p>In this package the default is just dense forward-mode AD (not-in place). But the function can easily be overloaded for your use case where you can pre-allocate, use sparse matrices with graph coloring (see SparseDiffTools.jl), and your own desired linear solver.</p><h4 id="Reverse-Mode"><a class="docs-heading-anchor" href="#Reverse-Mode">Reverse Mode</a><a id="Reverse-Mode-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-Mode" title="Permalink"></a></h4><p>If using reverse mode AD we have the derivatives of out outputs of interest with respect to y <span>$\bar{y}$</span> and need to propgate backwards to get <span>$\bar{x}$</span>.  Again, we&#39;re not really interested in <span>$J$</span>, but rather in <span>$\bar{x} = J^T \bar{y}$</span>.  First we take the transpose of our governing equation:</p><p class="math-container">\[J^T A^T = -B^T\]</p><p>Let&#39;s multiply both sides by some as yet unknown vector <span>$u$</span>.</p><p class="math-container">\[J^T A^T u = -B^T u\]</p><p>We now would like to solve for the value of <span>$u$</span> such that</p><p class="math-container">\[A^T u = \bar{y}\]</p><p>Doing so will give us the desired Jacobian vector product <span>$J^T \bar{y}$</span>:</p><p class="math-container">\[J^T A^T u = -B^T u\]</p><p class="math-container">\[J^T \bar{y} = -B^T u \]</p><p class="math-container">\[\bar{x} = -B^T u \]</p><p>In this case we start with solving the linear system:</p><p class="math-container">\[A^T u = \bar{y}\]</p><p>given input <span>$\bar{y}$</span>.  Again, we will want to use an appropriate factorization for A, sparsity, and appropriate linear solver.  The default implementation ses dense forward mode AD, and the linear solve is the same as before.</p><p>With u now known we compute the vector-Jacobian product <span>$\bar{x} = -B^T u = -(u^T B)^T$</span>.  Like the Jacobian-vector product of the previous section, we can compute this more efficiently by avoiding explicitly constructing the matrix B.  The vector-Jacobian product <span>$v^T \partial r/ \partial x$</span> can be computing with reverse-mode AD where the intial seed (adjoint vector) is set with the weights <span>$v$</span> (where <span>$v = -u$</span> in this case).</p><p>In this implementation, rather than go under the hood and manipulate the seed (since this function is not explicitly available), we can instead simply compute the gradient of:</p><p class="math-container">\[\frac{\partial }{\partial x}\left( v^T r(x, y) \right) = v^T \frac{\partial r}{\partial x}\]</p><p>which the expansion shows gives us the desired product. To repeat, we just compute the gradient of <span>$(v^T r)$</span>  with respect to <span>$x$</span> (note it is a gradient and not a Jacobian since we have a scalar output). This gives the desired vector, i.e., the derivatives <span>$\bar{x}$</span>.</p><p>In summary the operations are:</p><ol><li>Solve the linear system: <span>$A^T u = \bar{y}$</span> for the upstream input <span>$\bar{y}$</span> where <span>$A = \partial r / \partial y$</span>.</li><li>Compute the VJP: <span>$\bar{x} = -B^T u$</span> where <span>$B = \partial r/\partial x$</span>.</li></ol><h2 id="Linear-Equations"><a class="docs-heading-anchor" href="#Linear-Equations">Linear Equations</a><a id="Linear-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Equations" title="Permalink"></a></h2><p>For linear residuals the above nonlinear formulation will of course work, but we can provide partial derivatives symbolically.  This will be more efficient rather than relying on AD to compute these partials, or will make it easier on the user to not have to manually provide these.</p><h4 id="Forward-Mode-2"><a class="docs-heading-anchor" href="#Forward-Mode-2">Forward Mode</a><a class="docs-heading-anchor-permalink" href="#Forward-Mode-2" title="Permalink"></a></h4><p>Consider the linear system: <span>$A y = b$</span> where <span>$A$</span> and or <span>$b$</span> is a function of our input <span>$x$</span> and <span>$y$</span> is our state variables.  For our purposes <span>$A$</span> and <span>$b$</span> are the inputs to our function, the derivatives of <span>$A$</span> and <span>$b$</span> with respect to <span>$x$</span> are explicit and will already be computed as part of the forward mode AD.  The solution to the linear system is represented mathematically as:</p><p class="math-container">\[y = A^{-1} b\]</p><p>and we need <span>$\dot{y}$</span>, the derivatives of <span>$y$</span> with respect to our inputs of interest.  Using the chain rule we find:</p><p class="math-container">\[\dot{y} = \dot{A}^{-1} b + A^{-1} \dot{b}\]</p><p>We now need the derivative of a matix inverse, which we can find by considering its definition relative to an identity matrix:</p><p class="math-container">\[A A^{-1} = I\]</p><p>We now differentiate both sides:</p><p class="math-container">\[\dot{A} A^{-1} + A \dot{A}^{-1} = 0\]</p><p>and we can now solve for the derivative of the matrix inverse:</p><p class="math-container">\[\begin{aligned}
A \dot{A}^{-1} &amp;= - \dot{A} A^{-1} \\
 \dot{A}^{-1} &amp;= - A^{-1} \dot{A} A^{-1} 
\end{aligned}\]</p><p>We substitute this result into our above equation..</p><p class="math-container">\[\dot{y} = - A^{-1} \dot{A} A^{-1}  b + A^{-1} \dot{b}\]</p><p>We simplify this expression by noting that <span>$A^{-1}  b = y$</span> and factoring out the matrix inverse.</p><p class="math-container">\[\begin{aligned}
\dot{y} &amp;= - A^{-1} \dot{A} y + A^{-1} \dot{b}\\
\dot{y} &amp;= A^{-1} (-\dot{A} y + \dot{b})
\end{aligned}\]</p><p>This is the desired result:</p><p class="math-container">\[\dot{y} = A^{-1} (\dot{b} -\dot{A} y)\\\]</p><p>Both <span>$\dot{A}$</span> and <span>$\dot{b}$</span> are already known from the previous step in the forward AD process.  We simply extract those dual numbers from the inputs.  Or more conveniently we extract the partials from the vector: <span>$(b - Ay)$</span> since <span>$y$</span> is a constant, the solution to the linear system, and thus does not contain any dual numbers. Note that we should save the factorization of <span>$A$</span> from the primal linear solve to reuse in the equation above.</p><h4 id="Reverse-Mode-2"><a class="docs-heading-anchor" href="#Reverse-Mode-2">Reverse Mode</a><a class="docs-heading-anchor-permalink" href="#Reverse-Mode-2" title="Permalink"></a></h4><p>Reverse mode is a little trickier.  We need the derivatives of outputs of interest with respect to <span>$A$</span> and <span>$b$</span>.  For convenience, we consider one output at a time (though it works the same for multiple), and call this output <span>$\xi$</span>.  Notationally this is</p><p class="math-container">\[\bar{A} = \frac{d \xi}{dA},\, \bar{b} = \frac{d \xi}{db}\]</p><p>In this step of the reverse AD chain we would know <span>$\bar{y}$</span> the derivative of our output of interest with respect to <span>$y$</span>. </p><p>Computing the desired derivatives is easier in index notation since it produces third order tensors (e.g., derivative of a vector with respect to a matrix).  Let&#39;s go after the first derivative using the chain rule</p><p class="math-container">\[\bar{A}_{ij} = \frac{d \xi}{d A_{ij}} = \frac{d \xi}{d y_k} \frac{d y_k}{d A_{ij}}\]</p><p>We now need the derivative of the vector <span>$y$</span> with respect to <span>$A$</span>.  To get there we take derivatives of our governing equation (<span>$Ay = b$</span>):</p><p class="math-container">\[\frac{d}{d A_{ij}}  \left( A_{lm} y_m = b_l  \right)\]</p><p>The vector <span>$b$</span> is independent of <span>$A$</span> so we have:</p><p class="math-container">\[\begin{aligned}
\frac{d A_{lm}}{d A_{ij}} y_m + A_{lm} \frac{d y_m}{d A_{ij}}   &amp; = 0 \\
\delta_{li}\delta_{mj} y_m + A_{lm} \frac{d y_m}{A_{ij}}   &amp; = 0 \\
\delta_{li} y_j + A_{lm} \frac{d y_m}{A_{ij}}   &amp; = 0 \\
A_{nl}^{-1} \delta_{li} y_j + A_{nl}^{-1} A_{lm} \frac{d y_m}{A_{ij}}   &amp; = 0 \\
A_{nl}^{-1} \delta_{li} y_j + \delta_{nm} \frac{d y_m}{A_{ij}}   &amp; = 0 \\
A_{ni}^{-1}  y_j +  \frac{d y_n}{A_{ij}}   &amp; = 0 
\end{aligned}\]</p><p>Thus:</p><p class="math-container">\[\frac{d y_k}{A_{ij}} = -A_{ki}^{-1}  y_j  \]</p><p>We now substitute this result back into our above equation:</p><p class="math-container">\[\begin{aligned}
\bar{A}_{ij} &amp;= \bar{y}_k \frac{d y_k}{d A_{ij}} \\
\bar{A}_{ij} &amp;= -\bar{y}_k A_{ki}^{-1}  y_j   \\
\bar{A}_{ij} &amp;= - (A_{ik}^{-T}\bar{y}_k)  y_j   \\
\end{aligned}\]</p><p>Which we can recognize as an outer product</p><p class="math-container">\[\bar{A} - (A^{-T} \bar{y}) y^T\]</p><p>We now repeat the procedure for the derivatives with respect to <span>$b$</span></p><p class="math-container">\[\bar{b}_i = \frac{d \xi}{d b_i} = \frac{d \xi}{d y_j} \frac{d y_j}{d b_i} = \bar{y}_j \frac{d y_j}{d b_i}\]</p><p>We can easily get this last derivative</p><p class="math-container">\[\frac{d y_j}{d b_i} = \frac{d (A^{-1}_{jk} b_k)}{d b_i} = A^{-1}_{jk} \frac{d b_k}{d b_i} = A^{-1}_{jk} \delta_{ki} = A^{-1}_{ji} \]</p><p>Thus:</p><p class="math-container">\[\bar{b}_i = \bar{y}_j A^{-1}_{ji} \]</p><p>We now have the desired result</p><p class="math-container">\[\bar{b} = A^{-T} \bar{y}\]</p><p>From these results we see that we must first solve the linear system</p><p class="math-container">\[A^T u = \bar{y}\]</p><p>from which we easily get the desired derivatives for the revsere mode pullback operation</p><p class="math-container">\[\begin{aligned}
\bar{A} &amp;= -u y^T\\
\bar{b} &amp;= u
\end{aligned}\]</p><p>Note that we should again save the factorization for <span>$A$</span> from the primal solve to reuse in this second linear solve.</p><h2 id="Custom-Rules"><a class="docs-heading-anchor" href="#Custom-Rules">Custom Rules</a><a id="Custom-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Rules" title="Permalink"></a></h2><p>Consider now (typically explicit) functions of the form: <code>y = func(x, p)</code> where <code>x</code> are variables and <code>p</code> are fixed parameters.</p><h4 id="Forward"><a class="docs-heading-anchor" href="#Forward">Forward</a><a id="Forward-1"></a><a class="docs-heading-anchor-permalink" href="#Forward" title="Permalink"></a></h4><p>The derivatives we are after are:</p><p class="math-container">\[\frac{d y_i}{d \xi_j} = \frac{d y_i}{d x_k} \frac{d x_k}{d \xi_j}\]</p><p>or in our notation</p><p class="math-container">\[\dot{y} = J \dot{x}\]</p><p>where <span>$\dot{x}$</span> is known at this stage of the AD chain.</p><p>A user may be able to provide the Jacobian or the Jacobian-vector product directly.  If not, we can use finite differencing on complex step.  </p><p>A typical use case where one might resort to finite differencing is when calling a function from another language where we cannot propagate AD through (but where the rest of our code is AD-compatible and we want to insert the derivatives of this external call into the AD workflow).  Thus, we make the assumption that the function call <code>func</code> is expensive relative to the matrix operations.  </p><p>In the case of finite differencing and complex step we have a choice.  We are actually computing a Jacobian-Jacobian product, so we could either compute the Jacobian <span>$J$</span> first, which scales with the dimension of <span>$x$</span> (each index k from equation above).  Or, we can compute the Jacobian vector product for each <span>$\xi$</span>, which scales with the dimension of <span>$\xi$</span> (over index j).   We compare the size of <span>$x$</span> and <span>$\xi$</span> and choose the smaller number of function calls in this implementation.  This choice is rarely applicable in regular AD since <span>$n_x$</span> is generally only of size 1 or maybe 2 as rules are applied to individual operations.  But in this scenario the external call <code>func</code> is generally much more complex.</p><h4 id="Reverse"><a class="docs-heading-anchor" href="#Reverse">Reverse</a><a id="Reverse-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse" title="Permalink"></a></h4><p>Reverse is similar:</p><p class="math-container">\[\frac{d \xi_j}{d x_k} = \frac{d y_i}{d x_k} \frac{d xi_j}{d y_i}\]</p><p>or in our notation</p><p class="math-container">\[\bar{x} = J^T \bar{y}\]</p><p>where <span>$\bar{y}$</span> is known at this stage of the AD chain (though generally given for just one value of <span>$\xi$</span> at a time).</p><p>In this case the user can again provide the Jacobian or the vector-Jacobian product (v^T J), or we can use finite differencing on complex step.  </p><p>Finite differencing and complex step only work in a forward manner and so we cannot compute the VJP directly, the only option in revese mode is to construct the Jacobian via finite differencing and then multiply.  If we had the whole Jacobian <span>$\bar{x}$</span> available we could actually perform a JVP for each <span>$y_i$</span>.  However, the pullbacks only provide the opposite as noted above.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference/">« API</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 13 October 2022 18:11">Thursday 13 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
