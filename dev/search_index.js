var documenterSearchIndex = {"docs":
[{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can get to the derivatives we are after by using implicit differentiation. Recall that we are solving: r(x y(x)) = 0 and that we want dydx.  Using the chain rule we find that:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracdrdx = fracpartial rpartial x + fracpartial rpartial y fracdydx = 0","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"To be a properly solution, the residuals must remain at r = 0 with changes in the input.  Thus, the total derivative drdx must also be zero.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The Jacobian we are after we will call J = fracdydx.  The other two partial derivative Jacobians we will call A = fracpartial rpartial y and B = fracpartial rpartial x just for convenience.  The above equation then gives:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A J = -B","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Note that our desired Jacobian depends only on partial derivatives at the solution of our residual equations (and thus not on the solver path).  We can compute or provide these partial derivatives by various means, though typically we will leverage AD.","category":"page"},{"location":"theory/#Forward-Mode","page":"Theory","title":"Forward Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"If we are using forward mode algorithmic differentiation, once we reach this implicit function we already know the derivatives of x with respect to our variables of interest.  We will call this dotx.  The above equation described how we can compute the Jacobian J, but what we really want is not J but rather the Jacobian vector product J dotx, which gives us the derivatives of y with respect to our variables of interest, and we continue forward along the AD path.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We multiply both sides by dotx","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A J dotx = -B dotx","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"or ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A doty = -B dotx","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where doty is what we are after.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"First we deal with the right hand side computing:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"b = -B dotx","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can compute the resulting Jacobian vector product (Jacobian B times some vector v) without actually forming the matrix B, which is more efficient.  Recall that B = partial rpartial x.  If we want the Jacobian product B v, where v = -dotx in this case, we see that we are after a weighted sum of the columns sum_i partial rpartial x_i v_i where v_i are the weights.  Normally we pick off one of these partials in forward mode AD one at a time with different seeds e_i = 0 0 1 0, but if we set the initial seed to v_i then the partial derivatives we compute are precisely this weighted sum.  In other words, we initialize x as a dual number with its values as the input value x, and its derivatives as v, we evalute the residual function, then extract the partial derivatives which now give the desired Jacobian vector product b = B v (where v = -dotx).","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"With the right-hand side complete, we just need to compute the square Jacobian A = partial r  partial y and solve the linear system.  ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A dy = b","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Because A is square, forward mode AD is usually preferable for computing these partial derivatives. If we know the structure of A, we would want to provide an appropriate factorization of A.  Often, this Jacobian is sparse, and so using graph coloring is desirable.  Also for very large systems we can use matrix-free methods (e.g., Krylov methods) to solve the linear system without actually constructing A.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this package the default is just dense forward-mode AD (not-in place). But the function can easily be overloaded for your use case where you can pre-allocate, use sparse matrices, graph coloring (see SparseDiffTools.jl), and your own desired linear solver.","category":"page"},{"location":"theory/#Reverse-Mode","page":"Theory","title":"Reverse Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"If using reverse mode AD we have the derivatives of out outputs of interest with respect to y bary and need to propgate backwards to get barx.  Again, we're not really interested in J, but rather in barx = J^T bary.  First we take the transpose of our governing equation:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"J^T A^T = -B^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Let's multiply both sides by some as yet unknown vector u.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"J^T A^T u = -B^T u","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now would like to solve for the value of u such that","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A^T u = bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Doing so will give us the desired Jacobian vector product J^T bary:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"J^T A^T u = -B^T u","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"J^T bary = -B^T u ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barx = -B^T u ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this case we start with solving the linear system:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A^T u = bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"given input bary.  Again, we will want to use an appropriate factorization for A, sparsity, and appropriate linear solver.  The default implementation ses dense forward mode AD, and the linear solve is the same as before.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"With u now known we compute the vector-Jacobian product barx = -B^T u = -(u^T B)^T.  Like the Jacobian-vector product of the previous section, we can compute this more efficiently by avoiding explicitly constructing the matrix B.  The vector-Jacobian product v^T partial r partial x can be computing with reverse-mode AD where the intial seed (adjoint vector) is set with the weights v (where v = -u in this case).","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this implementation, rather than go under the hood and manipulate the seed (since this function is not explicitly available), we can instead simply compute the gradient of:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracpartial partial xleft( v^T r(x y) right) = v^T fracpartial rpartial x","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"which the expansion shows gives us the desired product. To repeat, we just compute the gradient of (v^T r)  with respect to x (note it is a gradient and not a Jacobian since we have a scalar output). This gives the desired vector, i.e., the derivatives barx.","category":"page"},{"location":"theory/#Linear-Equations","page":"Theory","title":"Linear Equations","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"For linear residuals the above nonlinear formulation will of course work, but we can provide partial derivatives symbolically.  This will be more efficient rather than relying on AD to compute these partials, or will make it easier on the user to not have to manually provide these.","category":"page"},{"location":"theory/#Forward-Mode-2","page":"Theory","title":"Forward Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Consider the linear system: A y = b where A and or b is a function of our input x and y is our state variables.  For our purposes A and b are the inputs to our function, the derivatives of A and b with respect to x are explicit and will already be computed as part of the forward mode AD.  The solution to the linear system is represented mathematically as:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"y = A^-1 b","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and we need doty, the derivatives of y with respect to our inputs of interest.  Using the chain rule we find:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"doty = dotA^-1 b + A^-1 dotb","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now need the derivative of a matix inverse, which we can find by considering its definition relative to an identity matrix:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A A^-1 = I","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now differentiate both sides:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"dotA A^-1 + A dotA^-1 = 0","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and we can now solve for the derivative of the matrix inverse:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nA dotA^-1 = - dotA A^-1 \n dotA^-1 = - A^-1 dotA A^-1 \nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We substitute this result into our above equation..","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"doty = - A^-1 dotA A^-1  b + A^-1 dotb","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We simplify this expression by noting that A^-1  b = y and factoring out the matrix inverse.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\ndoty = - A^-1 dotA y + A^-1 dotb\ndoty = A^-1 (-dotA y + dotb)\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This is the desired result:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"doty = A^-1 (dotb -dotA y)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Both dotA and dotb are already known from the previous step in the forward AD process.  We simply extract those dual numbers from the inputs.  Or more conveniently we extract the partials from the vector: (b - Ay) since y is a constant, the solution to the linear system, and thus does not contain any dual numbers. Note that we should save the factorization of A from the primal linear solve to reuse in the equation above.","category":"page"},{"location":"theory/#Reverse-Mode-2","page":"Theory","title":"Reverse Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Reverse mode is a little trickier.  We need the derivatives of outputs of interest with respect to A and b.  For convenience, we consider one output at a time (though it works the same for multiple), and call this output xi.  Notationally this is","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA = fracd xidA barb = fracd xidb","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this step of the reverse AD chain we would know bary the derivative of our output of interest with respect to y. ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Computing the desired derivatives is easier in index notation since it produces third order tensors (e.g., derivative of a vector with respect to a matrix).  Let's go after the first derivative using the chain rule","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA_ij = fracd xid A_ij = fracd xid y_k fracd y_kd A_ij","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now need the derivative of the vector y with respect to A.  To get there we take derivatives of our governing equation (Ay = b):","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracdd A_ij  left( A_lm y_m = b_l  right)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The vector b is independent of A so we have:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nfracd A_lmd A_ij y_m + A_lm fracd y_md A_ij    = 0 \ndelta_lidelta_mj y_m + A_lm fracd y_mA_ij    = 0 \ndelta_li y_j + A_lm fracd y_mA_ij    = 0 \nA_nl^-1 delta_li y_j + A_nl^-1 A_lm fracd y_mA_ij    = 0 \nA_nl^-1 delta_li y_j + delta_nm fracd y_mA_ij    = 0 \nA_ni^-1  y_j +  fracd y_nA_ij    = 0 \nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Thus:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd y_kA_ij = -A_ki^-1  y_j  ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now substitute this result back into our above equation:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nbarA_ij = bary_k fracd y_kd A_ij \nbarA_ij = -bary_k A_ki^-1  y_j   \nbarA_ij = - (A_ik^-Tbary_k)  y_j   \nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Which we can recognize as an outer product","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA - (A^-T bary) y^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now repeat the procedure for the derivatives with respect to b","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb_i = fracd xid b_i = fracd xid y_j fracd y_jd b_i = bary_j fracd y_jd b_i","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can easily get this last derivative","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd y_jd b_i = fracd (A^-1_jk b_k)d b_i = A^-1_jk fracd b_kd b_i = A^-1_jk delta_ki = A^-1_ji ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Thus:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb_i = bary_j A^-1_ji ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now have the desired result","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb = A^-T bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"From these results we see that we must first solve the linear system","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A^T u = bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"from which we easily get the desired derivatives for the revsere mode pullback operation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nbarA = -u y^T\nbarb = u\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Note that we should again save the factorization for A from the primal solve to reuse in this second linear solve.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Basic-Usage","page":"Examples","title":"Basic Usage","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's go through a complete example now. Assume we have two nonlinear implicit equations:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"r_1(x y) = (y_1 + x_1) (y_2^3 - x_2) + x_3 = 0\nr_2(x y) = sin(y_2 exp(y_1) - 1) x_4 = 0","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We will use the NLsolve package to solve these equations (refer to the first example in their documentation if not familiar with NLsolve).  We will also put explict operations before and after the solve just to show how this will work in the midst of a larger program.  ","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"\nusing NLsolve\n\nfunction residual!(r, x, y)\n    r[1] = (y[1] + x[1])*(y[2]^3-x[2])+x[3]\n    r[2] = sin(y[2]*exp(y[1])-1)*x[4]\nend\n\nfunction solve(x)\n    rwrap(r, y) = residual!(r, x[1:4], y)  # closure using some of the input variables within x just as an example\n    res = nlsolve(rwrap, [0.1; 1.2], autodiff=:forward)\n    return res.zero\nend\n\nfunction program(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = solve(w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now if we tried to run ForwardDiff.jacobian(program, x) it will not work.  It's not compatible with the internals of NLSolve, but even if it were it would be an inefficient way to compute the derivatives.  We now need to modify this script to use our package.  Here is what the modified program function will look like.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"\nusing ImplicitAD\n\nfunction modprogram(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = implicit_function(solve, residual!, w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"It is now both ForwardDiff and ReverseDiff compatible.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using ForwardDiff\nusing ReverseDiff\n\nx = [1.0; 2.0; 3.0; 4.0; 5.0]\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\nprintln(J1)\nprintln(\"max abs difference = \", maximum(abs.(J1 - J2)))","category":"page"},{"location":"examples/#Overloading-Subfunctions","page":"Examples","title":"Overloading Subfunctions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"As outlined in the math derivation, the forward mode consists of three main operations and custom implementations can be passed in.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"jvp(residual, x, y, v):  Compute the Jacobian vector product b = B*v where B_ij = partial r_ipartial x_j.  The default implementation uses forward mode AD where the Jacobian is not explicitly constructed (hence a Jacobian vector product).  This requires just evaluating the residual explicitly with dual numbers.\ndrdy(residual, x, y): Provide/compute or lazily instantiate partial r_ipartial y_j.  The default is forward mode AD.\nlsolve(A, b): Solve linear system A x = b where A is computed in drdy and b is computed in jvp.  The default is the backsplash operator.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In the reverse mode the operations are:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"drdy(residual, x, y): same as above.\nlsolve(A, b): same as above (although the passed in A is now the transpose of the matrix computed in drdy and b is a provided input).\nvjp(residual, x, y, v):  Compute the vector Jacobian product c = B^T v = (v^T B)^T where B_ij = partial r_ipartial x_j.  The default implementation uses reverse mode AD where the Jacobian is not explicitly constructed.  Instead only a gradient call is needed.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Note that in all of these subfunctions residual is of the explicit form: r = residual(x, y).  Since two of the functions are repeated, there are 4 functions that can be overriden if desired. Perhaps the most common would be to override drdy for cases where the Jacobian has significant sparsity, or is large so memory preallocation is important, or to apply a specific matrix factorization.  The linear solver lsolve might be overriden to use a Krylov method (in connection with using JVPs rather than an explicit drdy).  The functions jvp and vjp would be less commonly overriden, as they are efficient, but are available as needed.  There is also a keyword drdx where one can pass in a function of the same signature of drdy (but to compute partialrpartialx).  This is less commonly useful as both jvp and vjp would then use explicit matrix multiplication, but may be beneficial in some cases.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"As an example of custom subfunctions, let's continue the same problem from the previous section.  We note that we can provide the Jacobian partial rpartial y analytically and so we will skip the internal ForwardDiff implementation. We provide our own function for drdy, and we will preallocate so we can modify the Jacobian in place:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function drdy(residual, x, y, A)\n    A[1, 1] = y[2]^3-x[2]\n    A[1, 2] = 3*y[2]^2*(y[1]+x[1])\n    u = exp(y[1])*cos(y[2]*exp(y[1])-1)*x[4]\n    A[2, 1] = y[2]*u\n    A[2, 2] = u\n    return A\nend\nnothing # hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We can now pass this function in with a keyword argument to replace the default implementation for this subfunction.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"\nfunction modprogram2(x)\n    z = 2.0*x\n    w = z + x.^2\n    A = zeros(2, 2)\n    my_drdy(residual, x, y) = drdy(residual, x, y, A)\n    y = implicit_function(solve, residual!, w, drdy=my_drdy)\n    return y[1] .+ w*y[2]\nend\n\nJ3 = ForwardDiff.jacobian(modprogram2, x)\nprintln(maximum(abs.(J1 - J3)))\n","category":"page"},{"location":"examples/#Linear-residuals","page":"Examples","title":"Linear residuals","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"If the residuals are linear (i.e., Ay = b) we could still use the above nonlinear formulation but it will be inefficient or require more work from the user.  Instead, we can provide the partial derivatives analytically for the user.  In this case, the user need only provide the inputs A and b.  ","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's consider a simple example.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using SparseArrays: sparse\n\nfunction program(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = A \\ b\n    z = y.^2\n    return z\nend\nnothing # hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This function is actually not compatible with ForwardDiff because of the use of a sparse matrix (obviously unnecessary with such a small matrix, just for illustration).  We now modify this function using this package, with a one line change using implicit_linear_function, and can now compute the Jacobian.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's consider a simple example.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using ImplicitAD\nusing ForwardDiff\nusing ReverseDiff\n\nfunction modprogram(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = implicit_linear_function(A, b)\n    z = y.^2\n    return z\nend\n\nx = [1.0; 2.0; 3.0; 4.0]\n    \nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\n\nprintln(J1)\nprintln(maximum(abs.(J1 - J2)))","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For implicit_linear_function there are two keywords for custom subfunctions: ","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"lsolve(A, b): same purpose as before: solve A x = b where the default is the backslash operator.\nfact(A): provide a matrix factorization of A, since two linear solves are performed (for the primal and dual values).  default is factorize defined in LinearAlgebra.","category":"page"},{"location":"reference/#Reference","page":"API","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"API","title":"API","text":"There are just two exported function in this package, but each has multiple optional arguments.","category":"page"},{"location":"reference/","page":"API","title":"API","text":"implicit_function\nimplicit_linear_function","category":"page"},{"location":"reference/#ImplicitAD.implicit_function","page":"API","title":"ImplicitAD.implicit_function","text":"implicit_function(solve, residual, x; jvp=jvp_forward, vjp=vjp_reverse, drdx=nothing, drdy=drdy_forward, lsolve=linear_solve)\n\nMake implicit function AD compatible (specifically with ForwardDiff and ReverseDiff).\n\nArguments\n\nsolve::function: y = solve(x). Solve implicit function (where residual!(r, x, y) = 0, see below)\nresidual!::function: residual!(r, x, y). Set residual r, given input x and state y.\nx::vector{float}: evaluation point.\njvp::function: jvp(residual, x, y, v).  Compute Jacobian vector product b = B*v where Bij = ∂ri/∂x_j.  r = residual(x, y) (note explicit form.  Default leverages dual numbers in forward mode AD where Jacobian is not explicitly constructed.\nvjp::function: vjp(residual, x, y, v).  Compute vector Jacobian product c = B^T v = (v^T B)^T where Bij = ∂ri/∂x_j and return c  Default is reverse mode AD where Jacobian is not explicitly constructed.  Computes gradient of a scalar function.\ndrdx::function: drdx(residual, x, y).  Provide (or compute yourself): ∂ri/∂xj.  Not a required function. Default is nothing.  If provided, then jvp and jvp are ignored and explicitly multiplied against this provided Jacobian.\ndrdy::function: drdy(residual, x, y).  Provide (or compute yourself): ∂ri/∂yj.  Default is forward mode AD.\nlsolve::function: lsolve(A, b).  Linear solve A x = b  (where A is computed in drdy and b is computed in jvp, or it solves A^T x = c where c is computed in vjp).  Default is backslash operator.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.implicit_linear_function","page":"API","title":"ImplicitAD.implicit_linear_function","text":"implicit_linear_function(A, b; lsolve=linear_solve, fact=factorize)\n\nMake implicit function AD compatible (specifically with ForwardDiff and ReverseDiff). This version is for linear equations Ay = b\n\nArguments\n\nA::matrix, b::vector: components of linear system A y = b\nlsolve::function: lsolve(A, b). Function to solve the linear system, default is backslash operator.\nfact::function: fact(A).  Factorize matrix A and save it so we can reuse it for solving system and solving derivatives.  Default is factorize.\n\n\n\n\n\n","category":"function"},{"location":"#ImplicitAD-Documentation","page":"Home","title":"ImplicitAD Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Summary: Make implicit functions compatible with algorithmic differentiation without differenting inside the solvers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Author: Andrew Ning","category":"page"},{"location":"","page":"Home","title":"Home","text":"Features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compatible with ForwardDiff and ReverseDiff\nCompatible with any solver (no differentiation occurs inside the solver)\nOne line change for most cases\nCustomizable subfunctions to accomodate different use cases\nVersion for linear systems to provide symbolic partials automatically (again works with any linear solve whether or not it was already overloaded for AD)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Motivation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Many engineering analyses use implicit functions.  We can represent any such implicit function generally as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"r(y x) = 0","category":"page"},{"location":"","page":"Home","title":"Home","text":"where r are the residual functions we wish to drive to zero, x are inputs, and y are the state variables, which are also outputs once the system of equations is solved.  In other words, y is an implicit function of x. Note that all of these are vector quantities.  x is of size n_x, and r and y are of the same size n_r (must have equal number of unknowns and equations for a solution to exist). Graphically we could depict this relationship as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x –> r(y; x) –> y","category":"page"},{"location":"","page":"Home","title":"Home","text":"We then chose some appropriate solver to converge these residuals.  From a differentiation perspective, we would like to compute dydx.  One can often use algorithmic differentiation (AD) in the same way one would for any explicit function.  Once we unroll the iterations of the solver the set of instructions is explicit.  However, this is at best inefficient and at worse inaccurate or not possible (at least not without a lot more effort).  To obtain accurate derivatives by propgating AD through a solver, the solver must be solved to a tight tolerance.  Generally tighter than is required to converge the primal values.  Sometimes this is not feasible because operations inside the solvers may not be overloaded for AD, this is especially true when calling solvers in other languages.  But even if we can do it (tight convergence is possible and everything under the hood is overloaded) we usually still shouldn't, as it would be computationally inefficient.  Instead we can use implicit differentiation, to allow for AD to work seemlessly with implicit functions without having to differentiate through them.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides an implementation so that a simple one-line change can be applied to allow AD to be propgated around any solver.  Note that the implementation of the solver need not be AD compatible since AD does not not occur inside the solver.  This package is overloaded for ForwardDiff.jl and ReverseDiff.jl.  There are also optional inputs so that subfunction behavior can be customized (e.g., preallocation, custom linear solvers, custom factorizations, custom jacobian vector products, etc.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Start with the quick start tutorial to learn basic usage.\nThe API is described in the reference page.\nThe math is particularly helpful for those wanting to provide their own custom subfunctions. See the theory page.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Run Unit Tests:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> activate .\npkg> test","category":"page"},{"location":"tutorial/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"We can generically represent the solver that converges the residuals and computes the corresponding state variables as:","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"y = solve(x)","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Our larger code may then have a mix of explicit and implicit functions","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"function example(a)\n    b = 2*a\n    x = @. exp(b) + a\n    y = solve(x)\n    z = sqrt.(y)\n    return z\nend","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"To make this function compatible we only need to replace the call to solve with an overloaded function implicit_function defined in this module:","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"using ImplicitAD\n\nfunction example(a)\n    b = 2*a\n    x = @. exp(b) + a\n    y = implicit_function(solve, residual, x)\n    z = sqrt.(y)\n    return z\nend","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Note that the only new piece of information we need to expose is the residual function: residual(r, x, y), so that partial derivatives can be computed.  The new function is now compatible with ForwardDiff or ReverseDiff, for any solver, and efficiently provides the correct derivatives without differentiating inside the solver.","category":"page"},{"location":"tutorial/#Basic-Usage","page":"Quick Start","title":"Basic Usage","text":"","category":"section"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Let's go through a complete example now. Assume we have two nonlinear implicit equations:","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"r_1(x y) = (y_1 + x_1) (y_2^3 - x_2) + x_3 = 0\nr_2(x y) = sin(y_2 exp(y_1) - 1) x_4 = 0","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"We will use the NLsolve package to solve these equations (refer to the first example in their documentation if not familiar with NLsolve).  We will also put explict operations before and after the solve just to show how this will work in the midst of a larger program.  ","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"\nusing NLsolve\n\nfunction residual!(r, x, y)\n    r[1] = (y[1] + x[1])*(y[2]^3-x[2])+x[3]\n    r[2] = sin(y[2]*exp(y[1])-1)*x[4]\nend\n\nfunction solve(x)\n    rwrap(r, y) = residual!(r, x[1:4], y)  # closure using some of the input variables within x just as an example\n    res = nlsolve(rwrap, [0.1; 1.2], autodiff=:forward)\n    return res.zero\nend\n\nfunction program(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = solve(w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Now if we tried to run ForwardDiff.jacobian(program, x) it will not work.  It's not compatible with the internals of NLSolve, but even if it were it would be an inefficient way to compute the derivatives.  We now need to modify this script to use our package.  Here is what the modified program function will look like.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"\nusing ImplicitAD\n\nfunction modprogram(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = implicit_function(solve, residual!, w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"It is now both ForwardDiff and ReverseDiff compatible.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"using ForwardDiff\nusing ReverseDiff\n\nx = [1.0; 2.0; 3.0; 4.0; 5.0]\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\nprintln(J1)\nprintln(\"max abs difference = \", maximum(abs.(J1 - J2)))","category":"page"},{"location":"tutorial/#Overloading-Subfunctions","page":"Quick Start","title":"Overloading Subfunctions","text":"","category":"section"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"As outlined in the math derivation, the forward mode consists of three main operations and custom implementations can be passed in.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"jvp(residual, x, y, v):  Compute the Jacobian vector product b = B*v where B_ij = partial r_ipartial x_j.  The default implementation uses forward mode AD where the Jacobian is not explicitly constructed (hence a Jacobian vector product).  This requires just evaluating the residual explicitly with dual numbers.\ndrdy(residual, x, y): Provide/compute or lazily instantiate partial r_ipartial y_j.  The default is forward mode AD.\nlsolve(A, b): Solve linear system A x = b where A is computed in drdy and b is computed in jvp.  The default is the backsplash operator.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"In the reverse mode the operations are:","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"drdy(residual, x, y): same as above.\nlsolve(A, b): same as above (although the passed in A is now the transpose of the matrix computed in drdy and b is a provided input).\nvjp(residual, x, y, v):  Compute the vector Jacobian product c = B^T v = (v^T B)^T where B_ij = partial r_ipartial x_j.  The default implementation uses reverse mode AD where the Jacobian is not explicitly constructed.  Instead only a gradient call is needed.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Note that in all of these subfunctions residual is of the explicit form: r = residual(x, y).  Since two of the functions are repeated, there are 4 functions that can be overriden if desired. Perhaps the most common would be to override drdy for cases where the Jacobian has significant sparsity, or is large so memory preallocation is important, or to apply a specific matrix factorization.  The linear solver lsolve might be overriden to use a Krylov method (in connection with using JVPs rather than an explicit drdy).  The functions jvp and vjp would be less commonly overriden, as they are efficient, but are available as needed.  There is also a keyword drdx where one can pass in a function of the same signature of drdy (but to compute partialrpartialx).  This is less commonly useful as both jvp and vjp would then use explicit matrix multiplication, but may be beneficial in some cases.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"As an example of custom subfunctions, let's continue the same problem from the previous section.  We note that we can provide the Jacobian partial rpartial y analytically and so we will skip the internal ForwardDiff implementation. We provide our own function for drdy, and we will preallocate so we can modify the Jacobian in place:","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"function drdy(residual, x, y, A)\n    A[1, 1] = y[2]^3-x[2]\n    A[1, 2] = 3*y[2]^2*(y[1]+x[1])\n    u = exp(y[1])*cos(y[2]*exp(y[1])-1)*x[4]\n    A[2, 1] = y[2]*u\n    A[2, 2] = u\n    return A\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"We can now pass this function in with a keyword argument to replace the default implementation for this subfunction.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"\nfunction modprogram2(x)\n    z = 2.0*x\n    w = z + x.^2\n    A = zeros(2, 2)\n    my_drdy(residual, x, y) = drdy(residual, x, y, A)\n    y = implicit_function(solve, residual!, w, drdy=my_drdy)\n    return y[1] .+ w*y[2]\nend\n\nJ3 = ForwardDiff.jacobian(modprogram2, x)\nprintln(maximum(abs.(J1 - J3)))\n","category":"page"},{"location":"tutorial/#Linear-residuals","page":"Quick Start","title":"Linear residuals","text":"","category":"section"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"If the residuals are linear (i.e., Ay = b) we could still use the above nonlinear formulation but it will be inefficient or require more work from the user.  Instead, we can provide the partial derivatives analytically for the user.  In this case, the user need only provide the inputs A and b.  ","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Let's consider a simple example.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"using SparseArrays: sparse\n\nfunction program(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = A \\ b\n    z = y.^2\n    return z\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"This function is actually not compatible with ForwardDiff because of the use of a sparse matrix (obviously unnecessary with such a small matrix, just for illustration).  We now modify this function using this package, with a one line change using implicit_linear_function, and can now compute the Jacobian.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"Let's consider a simple example.","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"using ImplicitAD\nusing ForwardDiff\nusing ReverseDiff\n\nfunction modprogram(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = implicit_linear_function(A, b)\n    z = y.^2\n    return z\nend\n\nx = [1.0; 2.0; 3.0; 4.0]\n    \nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\n\nprintln(J1)\nprintln(maximum(abs.(J1 - J2)))","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"For implicit_linear_function there are two keywords for custom subfunctions: ","category":"page"},{"location":"tutorial/","page":"Quick Start","title":"Quick Start","text":"lsolve(A, b): same purpose as before: solve A x = b where the default is the backslash operator.\nfact(A): provide a matrix factorization of A, since two linear solves are performed (for the primal and dual values).  default is factorize defined in LinearAlgebra.","category":"page"}]
}
